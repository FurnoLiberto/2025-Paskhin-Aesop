# Генерация текста в стиле Эзопа

Проект реализует генерацию текста в стиле басен Эзопа с использованием рекуррентной нейронной сети (LSTM) на PyTorch.

## Архитектура модели
Реализована архитектура `Embedding -> LSTM -> Dense`, соответствующая заданию:
*   **Embedding Layer**: Вектор размерностью 100.
*   **LSTM Layer**: Hidden size 256.
*   **Dense Layer**: Проекция на размер словаря.

## Данные
Используется текст книги Aesop's Fables (Project Gutenberg ID 21).
*   Источник: [gutenberg.org](https://www.gutenberg.org/ebooks/21)
*   Предобработка:
    *   Удаление служебной информации.
    *   Приведение к нижнему регистру.
    *   Разделение историй специальным токеном `|||||||||||||||||||`.

## Запуск
1. Загрузка данных: `bash download_data.sh 21 aesop`
2. Обучение: `python src/train.py`
3. Генерация: `python src/inference.py`

## Результаты обучения
Обучение проводилось в течение 20 эпох.
Мониторинг осуществлялся через TensorBoard.

<img width="872" height="463" alt="изображение" src="https://github.com/user-attachments/assets/6ae453af-deda-46ee-8d30-5aeeef1e908c" />

<img width="873" height="429" alt="изображение" src="https://github.com/user-attachments/assets/ffe682c2-a3fa-4ca2-b3d9-bf808083de99" />

Средний Loss(train): 5.47
### Выводы
Модель показывает снижение функции потерь (Loss), что говорит об успешном обучении. 

Сгенерированный текст имитирует структуру предложений басен, однако из-за простой пословной токенизации и ограниченного размера датасета, смысловая нагрузка на длинных дистанциях может теряться.
