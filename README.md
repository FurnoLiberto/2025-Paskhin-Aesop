# Генерация текста в стиле Эзопа

Проект реализует генерацию текста в стиле басен Эзопа с использованием рекуррентной нейронной сети (LSTM) на PyTorch.

## Архитектура модели
Реализована архитектура `Embedding -> LSTM -> Dense`, соответствующая заданию:
*   **Embedding Layer**: Вектор размерностью 100.
*   **LSTM Layer**: Hidden size 256.
*   **Dense Layer**: Проекция на размер словаря.

## Данные
Используется текст книги Aesop's Fables (Project Gutenberg ID 21).
*   Источник: [gutenberg.org](https://www.gutenberg.org/ebooks/21)
*   Предобработка:
    *   Удаление служебной информации.
    *   Приведение к нижнему регистру.
    *   Разделение историй специальным токеном `|||||||||||||||||||`.

## Запуск
1. Загрузка данных: `bash download_data.sh 21 aesop`
2. Обучение: `python src/train.py`
3. Генерация: `python src/inference.py`

## Результаты обучения
Обучение проводилось в течение 20 эпох.
Мониторинг осуществлялся через TensorBoard.

<img width="872" height="463" alt="изображение" src="https://github.com/user-attachments/assets/6ae453af-deda-46ee-8d30-5aeeef1e908c" />

<img width="873" height="429" alt="изображение" src="https://github.com/user-attachments/assets/ffe682c2-a3fa-4ca2-b3d9-bf808083de99" />

Средний Loss(train): 5.47
### Выводы
После 5-й эпохи модель перестала понимать структуру языка и начала просто заучивать наизусть куски текста из тренировочной выборки. Чем лучше она зубрит (розовая вниз), тем хуже она работает на новых текстах (желтая вверх).

Почему это произошло?

Басни Эзопа — это очень маленький объем текста. Модели LSTM (особенно с 256 скрытыми нейронами) хватило памяти, чтобы просто запомнить его целиком.
20 эпох для такого объема данных — это слишком много.

Что делать?

Глядя на этот график, мы понимаем, что идеальная модель была сохранена примерно на 4-й или 5-й эпохе. Всё, что было дальше — вредно.

 В коде src/train.py есть строчка:

```python
if avg_val_loss < best_val_loss:
    torch.save(model.state_dict(), "aesop_best_model.pth")
```
Благодаря ей, на диске сохранился файл aesop_best_model.pth. Это веса именно из той "ямы" желтой линии (эпоха 4-5).
А файл aesop_final.pth (эпоха 20) — это "переобученная" версия, которая будет генерировать текст хуже (скорее всего, просто цитировать куски).

Сгенерированный текст имитирует структуру предложений басен, однако из-за простой пословной токенизации и ограниченного размера датасета, смысловая нагрузка на длинных дистанциях может теряться.
