# Генерация текста в стиле Эзопа

Проект реализует генерацию текста в стиле басен Эзопа с использованием рекуррентной нейронной сети (LSTM) на PyTorch.

## Архитектура модели
Реализована архитектура `Embedding -> LSTM -> Dense`, соответствующая заданию:
*   **Embedding Layer**: Вектор размерностью 100.
*   **LSTM Layer**: Hidden size 256.
*   **Dense Layer**: Проекция на размер словаря.

## Данные
Используется текст книги Aesop's Fables (Project Gutenberg ID 21).
*   Источник: [gutenberg.org](https://www.gutenberg.org/ebooks/21)
*   Предобработка:
    *   Удаление служебной информации.
    *   Приведение к нижнему регистру.
    *   Разделение историй специальным токеном `|||||||||||||||||||`.

## Результаты обучения
Обучение проводилось в течение 5 эпох.
Мониторинг осуществлялся через TensorBoard.

<img width="891" height="427" alt="изображение" src="https://github.com/user-attachments/assets/68a5d400-0e92-4d94-beaa-2ca36352ded9" />

Средний Loss: 4.8829
### Выводы
Модель показывает снижение функции потерь (Loss), что говорит об успешном обучении. 
Сгенерированный текст имитирует структуру предложений басен, однако из-за простой пословной токенизации и ограниченного размера датасета, смысловая нагрузка на длинных дистанциях может теряться.

## Запуск
1. Загрузка данных: `bash download_data.sh 21 aesop`
2. Обучение: `python src/train.py`
3. Генерация: `python src/inference.py`
